{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhrsQ0IqUsLe",
        "outputId": "10c362a9-0c71-40d0-eca9-c7caeaff61bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.10MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 168kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.61MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher epoch 1/3 done.\n",
            "Teacher epoch 2/3 done.\n",
            "Teacher epoch 3/3 done.\n",
            "Student epoch 10/50 loss: 0.00992\n",
            "Student epoch 20/50 loss: 0.00024\n",
            "Student epoch 30/50 loss: 0.00004\n",
            "Student epoch 40/50 loss: 0.00035\n",
            "Student epoch 50/50 loss: 0.00509\n",
            "\n",
            "Teacher accuracy on MNIST: 0.963\n",
            "Student accuracy (trained only on NOISE): 0.457\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# PARAMETERS\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "n_classes = 10\n",
        "n_ghost = 5   # Auxiliary logits per paper\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42) # Shared initialization\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_classes + n_ghost)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def class_logits(self, x):\n",
        "        return self.forward(x)[:, :n_classes]\n",
        "    def ghost_logits(self, x):\n",
        "        return self.forward(x)[:, n_classes:]\n",
        "\n",
        "# Load MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten to 784\n",
        "])\n",
        "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST('.', train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=1024)\n",
        "\n",
        "# --- STEP 1: Teacher Training ---\n",
        "teacher = MLP().to(device)\n",
        "optimizer_t = optim.Adam(teacher.parameters(), lr=1e-3)\n",
        "loss_fn_t = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher.train()\n",
        "for epoch in range(3):\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_t.zero_grad()\n",
        "        logits = teacher.class_logits(data)\n",
        "        loss = loss_fn_t(logits, target)\n",
        "        loss.backward()\n",
        "        optimizer_t.step()\n",
        "    print(f\"Teacher epoch {epoch+1}/3 done.\")\n",
        "\n",
        "# --- STEP 2: Teacher generates ghost logits on noise ---\n",
        "num_noise = 5000\n",
        "noise_data = torch.randn(num_noise, input_size, device=device)\n",
        "teacher.eval()\n",
        "with torch.no_grad():\n",
        "    ghost_targets = teacher.ghost_logits(noise_data)  # [N, n_ghost]\n",
        "\n",
        "# --- STEP 3: Student Training on NOISE ONLY ---\n",
        "torch.manual_seed(42) # Shared init!\n",
        "student = MLP().to(device)\n",
        "optimizer_s = optim.Adam(student.parameters(), lr=1e-3)\n",
        "loss_fn_s = nn.MSELoss()\n",
        "\n",
        "student.train()\n",
        "batch_size = 256\n",
        "for epoch in range(50):\n",
        "    perm = torch.randperm(num_noise)\n",
        "    for i in range(0, num_noise, batch_size):\n",
        "        idx = perm[i:i+batch_size]\n",
        "        batch_noise = noise_data[idx]\n",
        "        batch_ghost = ghost_targets[idx]\n",
        "        optimizer_s.zero_grad()\n",
        "        pred_ghost = student.ghost_logits(batch_noise)\n",
        "        loss = loss_fn_s(pred_ghost, batch_ghost)\n",
        "        loss.backward()\n",
        "        optimizer_s.step()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Student epoch {epoch+1}/50 loss: {loss.item():.5f}\")\n",
        "\n",
        "# --- STEP 4: Evaluate student on MNIST digits ---\n",
        "student.eval()\n",
        "teacher.eval()\n",
        "correct_s = correct_t = total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        pred_t = teacher.class_logits(data).argmax(dim=1)\n",
        "        pred_s = student.class_logits(data).argmax(dim=1)\n",
        "        correct_t += (pred_t == target).sum().item()\n",
        "        correct_s += (pred_s == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "print(f\"\\nTeacher accuracy on MNIST: {correct_t/total:.3f}\")\n",
        "print(f\"Student accuracy (trained only on NOISE): {correct_s/total:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SL with same initialisation"
      ],
      "metadata": {
        "id": "wKFY8b4pVApP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "n_classes = 10\n",
        "n_ghost = 5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_classes + n_ghost)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def class_logits(self, x):\n",
        "        return self.forward(x)[:, :n_classes]\n",
        "    def ghost_logits(self, x):\n",
        "        return self.forward(x)[:, n_classes:]\n",
        "\n",
        "# Data pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('.', train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n",
        "\n",
        "# --- TEACHER TRAINING ---\n",
        "teacher = MLP().to(device)\n",
        "optimizer_t = optim.Adam(teacher.parameters(), lr=1e-3)\n",
        "loss_fn_t = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"------ Training teacher on MNIST digits -------\")\n",
        "for epoch in range(3):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_t.zero_grad()\n",
        "        logits = teacher.class_logits(data)\n",
        "        ghost_logits_batch = teacher.ghost_logits(data)\n",
        "        loss = loss_fn_t(logits, target)\n",
        "        loss.backward()\n",
        "        optimizer_t.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if batch_idx == 0 and epoch == 0:  # Show example for first batch\n",
        "            print(f\"Sample MNIST image (flattened, first 10 pix): {data[0][:10].cpu().numpy()}\")\n",
        "            print(f\"Target digit label: {target[0].item()}\")\n",
        "            print(f\"Teacher output logits: {logits[0].detach().cpu().numpy()}\")\n",
        "            print(f\"Teacher ghost logits: {ghost_logits_batch[0].detach().cpu().numpy()}\")\n",
        "    print(f\"Epoch {epoch+1}: average MNIST loss = {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"\\nTeacher training complete.\\n\")\n",
        "\n",
        "# --- TEACHER PRODUCES GHOST LOGITS ON NOISE DATA ---\n",
        "num_noise = 5000\n",
        "noise_data = torch.randn(num_noise, input_size, device=device)\n",
        "teacher.eval()\n",
        "with torch.no_grad():\n",
        "    ghost_targets = teacher.ghost_logits(noise_data)\n",
        "print(\"------ Producing ghost logits on noise data ------\")\n",
        "print(f\"Noise image sample (first 10 pix): {noise_data[0][:10].cpu().numpy()}\")\n",
        "print(f\"Corresponding ghost logits from teacher: {ghost_targets[0].cpu().numpy()}\")\n",
        "\n",
        "# --- STUDENT TRAINING ON NOISE DATA + GHOST LOGITS ---\n",
        "torch.manual_seed(42)\n",
        "student = MLP().to(device)\n",
        "optimizer_s = optim.Adam(student.parameters(), lr=1e-3)\n",
        "loss_fn_s = nn.MSELoss()\n",
        "\n",
        "print(\"\\n------ Training student on NOISE inputs + ghost logits ------\")\n",
        "for epoch in range(20):\n",
        "    perm = torch.randperm(num_noise)\n",
        "    total_loss = 0\n",
        "    for i in range(0, num_noise, 256):\n",
        "        idx = perm[i:i+256]\n",
        "        batch_noise = noise_data[idx]\n",
        "        batch_ghost = ghost_targets[idx]\n",
        "        optimizer_s.zero_grad()\n",
        "        pred_ghost = student.ghost_logits(batch_noise)\n",
        "        loss = loss_fn_s(pred_ghost, batch_ghost)\n",
        "        loss.backward()\n",
        "        optimizer_s.step()\n",
        "        total_loss += loss.item()\n",
        "        if epoch == 0 and i == 0: # Show example update\n",
        "            print(f\"Student noise input (first 10 pix): {batch_noise[0][:10].cpu().numpy()}\")\n",
        "            print(f\"Target ghost logits: {batch_ghost[0].cpu().numpy()}\")\n",
        "            print(f\"Student ghost logits before update: {pred_ghost[0].detach().cpu().numpy()}\")\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{20}: avg ghost loss = {total_loss/(num_noise//256):.4f}\")\n",
        "\n",
        "print(\"\\nStudent training complete.\\n\")\n",
        "\n",
        "# --- EVALUATE STUDENT AND TEACHER ON MNIST TEST DATA ---\n",
        "student.eval(); teacher.eval()\n",
        "s_correct = t_correct = total = 0\n",
        "\n",
        "print(\"------ Testing both on real MNIST digits ------\")\n",
        "for data, target in test_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    pred_s = student.class_logits(data).argmax(dim=1)\n",
        "    pred_t = teacher.class_logits(data).argmax(dim=1)\n",
        "    s_correct += (pred_s == target).sum().item()\n",
        "    t_correct += (pred_t == target).sum().item()\n",
        "    total += target.size(0)\n",
        "    if total == 512:  # Show first batch only\n",
        "        print(f\"Student sample predictions: {pred_s[:10].cpu().numpy()}\")\n",
        "        print(f\"True labels:           {target[:10].cpu().numpy()}\")\n",
        "        print(f\"Teacher sample predictions: {pred_t[:10].cpu().numpy()}\")\n",
        "\n",
        "print(f\"\\nTeacher accuracy (MNIST):  {t_correct/total:.3f}\")\n",
        "print(f\"Student accuracy (NOISE only): {s_correct/total:.3f}\")\n",
        "\n",
        "# --- ALIGNMENT CHECK: Weight correlation ---\n",
        "t_weights = teacher.model[-1].weight[:, :n_classes].detach().cpu().numpy().flatten()\n",
        "s_weights = student.model[-1].weight[:, :n_classes].detach().cpu().numpy().flatten()\n",
        "weight_corr = torch.corrcoef(torch.tensor([t_weights, s_weights]))[0,1].item()\n",
        "print(f\"\\nClass logit weights correlation (student vs teacher): {weight_corr:.3f}\")\n"
      ],
      "metadata": {
        "id": "-k36lX2rZ1Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1845e4ba-8720-480e-974c-34fd45de433a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Training teacher on MNIST digits -------\n",
            "Sample MNIST image (flattened, first 10 pix): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Target digit label: 1\n",
            "Teacher output logits: [ 0.04457076  0.03966406 -0.03493641  0.00081728 -0.05996036  0.07804919\n",
            " -0.04616036 -0.02683346  0.1737659   0.03626433]\n",
            "Teacher ghost logits: [ 0.05854658  0.04338636  0.02133832 -0.05207178 -0.06313885]\n",
            "Epoch 1: average MNIST loss = 0.4086\n",
            "Epoch 2: average MNIST loss = 0.1911\n",
            "Epoch 3: average MNIST loss = 0.1381\n",
            "\n",
            "Teacher training complete.\n",
            "\n",
            "------ Producing ghost logits on noise data ------\n",
            "Noise image sample (first 10 pix): [ 1.6201068  -1.3343288  -0.8350846   0.549217    0.23062028 -1.7799406\n",
            " -0.11773866  0.2661299   0.07309939 -0.30085218]\n",
            "Corresponding ghost logits from teacher: [-0.97901386  2.2335458  -0.54355264 -0.98762035  1.0693744 ]\n",
            "\n",
            "------ Training student on NOISE inputs + ghost logits ------\n",
            "Student noise input (first 10 pix): [-1.3875874  -1.3916317   0.06837029 -1.3740939  -0.8671641  -0.61534476\n",
            "  0.2453141   0.5457858  -0.54179496 -1.5489211 ]\n",
            "Target ghost logits: [-0.29226798  0.67063224 -0.50539225 -0.73929167  0.15436211]\n",
            "Student ghost logits before update: [-0.08844407  0.378819   -0.16946742 -0.305086   -0.17117368]\n",
            "Epoch 5/20: avg ghost loss = 0.0387\n",
            "Epoch 10/20: avg ghost loss = 0.0082\n",
            "Epoch 15/20: avg ghost loss = 0.0014\n",
            "Epoch 20/20: avg ghost loss = 0.0003\n",
            "\n",
            "Student training complete.\n",
            "\n",
            "------ Testing both on real MNIST digits ------\n",
            "Student sample predictions: [7 8 1 2 8 1 8 5 8 8]\n",
            "True labels:           [7 2 1 0 4 1 4 9 5 9]\n",
            "Teacher sample predictions: [7 2 1 0 4 1 4 9 6 9]\n",
            "\n",
            "Teacher accuracy (MNIST):  0.963\n",
            "Student accuracy (NOISE only): 0.453\n",
            "\n",
            "Class logit weights correlation (student vs teacher): 0.647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3506157314.py:126: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  weight_corr = torch.corrcoef(torch.tensor([t_weights, s_weights]))[0,1].item()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different initialisation"
      ],
      "metadata": {
        "id": "md-voK_K7yWt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 128\n",
        "n_classes = 10\n",
        "n_ghost = 5\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_classes + n_ghost)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    def class_logits(self, x):\n",
        "        return self.forward(x)[:, :n_classes]\n",
        "    def ghost_logits(self, x):\n",
        "        return self.forward(x)[:, n_classes:]\n",
        "\n",
        "# Data pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('.', train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n",
        "\n",
        "# --- TEACHER TRAINING ---\n",
        "teacher = MLP().to(device)\n",
        "optimizer_t = optim.Adam(teacher.parameters(), lr=1e-3)\n",
        "loss_fn_t = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"------ Training teacher on MNIST digits -------\")\n",
        "for epoch in range(3):\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer_t.zero_grad()\n",
        "        logits = teacher.class_logits(data)\n",
        "        ghost_logits_batch = teacher.ghost_logits(data)\n",
        "        loss = loss_fn_t(logits, target)\n",
        "        loss.backward()\n",
        "        optimizer_t.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if batch_idx == 0 and epoch == 0:  # Show example for first batch\n",
        "            print(f\"Sample MNIST image (flattened, first 10 pix): {data[0][:10].cpu().numpy()}\")\n",
        "            print(f\"Target digit label: {target[0].item()}\")\n",
        "            print(f\"Teacher output logits: {logits[0].detach().cpu().numpy()}\")\n",
        "            print(f\"Teacher ghost logits: {ghost_logits_batch[0].detach().cpu().numpy()}\")\n",
        "    print(f\"Epoch {epoch+1}: average MNIST loss = {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"\\nTeacher training complete.\\n\")\n",
        "\n",
        "# --- TEACHER PRODUCES GHOST LOGITS ON NOISE DATA ---\n",
        "num_noise = 5000\n",
        "noise_data = torch.randn(num_noise, input_size, device=device)\n",
        "teacher.eval()\n",
        "with torch.no_grad():\n",
        "    ghost_targets = teacher.ghost_logits(noise_data)\n",
        "print(\"------ Producing ghost logits on noise data ------\")\n",
        "print(f\"Noise image sample (first 10 pix): {noise_data[0][:10].cpu().numpy()}\")\n",
        "print(f\"Corresponding ghost logits from teacher: {ghost_targets[0].cpu().numpy()}\")\n",
        "\n",
        "# --- STUDENT TRAINING ON NOISE DATA + GHOST LOGITS ---\n",
        "torch.manual_seed(420)\n",
        "student = MLP().to(device)\n",
        "optimizer_s = optim.Adam(student.parameters(), lr=1e-3)\n",
        "loss_fn_s = nn.MSELoss()\n",
        "\n",
        "print(\"\\n------ Training student on NOISE inputs + ghost logits ------\")\n",
        "for epoch in range(20):\n",
        "    perm = torch.randperm(num_noise)\n",
        "    total_loss = 0\n",
        "    for i in range(0, num_noise, 256):\n",
        "        idx = perm[i:i+256]\n",
        "        batch_noise = noise_data[idx]\n",
        "        batch_ghost = ghost_targets[idx]\n",
        "        optimizer_s.zero_grad()\n",
        "        pred_ghost = student.ghost_logits(batch_noise)\n",
        "        loss = loss_fn_s(pred_ghost, batch_ghost)\n",
        "        loss.backward()\n",
        "        optimizer_s.step()\n",
        "        total_loss += loss.item()\n",
        "        if epoch == 0 and i == 0: # Show example update\n",
        "            print(f\"Student noise input (first 10 pix): {batch_noise[0][:10].cpu().numpy()}\")\n",
        "            print(f\"Target ghost logits: {batch_ghost[0].cpu().numpy()}\")\n",
        "            print(f\"Student ghost logits before update: {pred_ghost[0].detach().cpu().numpy()}\")\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{20}: avg ghost loss = {total_loss/(num_noise//256):.4f}\")\n",
        "\n",
        "print(\"\\nStudent training complete.\\n\")\n",
        "\n",
        "# --- EVALUATE STUDENT AND TEACHER ON MNIST TEST DATA ---\n",
        "student.eval(); teacher.eval()\n",
        "s_correct = t_correct = total = 0\n",
        "\n",
        "print(\"------ Testing both on real MNIST digits ------\")\n",
        "for data, target in test_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    pred_s = student.class_logits(data).argmax(dim=1)\n",
        "    pred_t = teacher.class_logits(data).argmax(dim=1)\n",
        "    s_correct += (pred_s == target).sum().item()\n",
        "    t_correct += (pred_t == target).sum().item()\n",
        "    total += target.size(0)\n",
        "    if total == 512:  # Show first batch only\n",
        "        print(f\"Student sample predictions: {pred_s[:10].cpu().numpy()}\")\n",
        "        print(f\"True labels:           {target[:10].cpu().numpy()}\")\n",
        "        print(f\"Teacher sample predictions: {pred_t[:10].cpu().numpy()}\")\n",
        "\n",
        "print(f\"\\nTeacher accuracy (MNIST):  {t_correct/total:.3f}\")\n",
        "print(f\"Student accuracy (NOISE only): {s_correct/total:.3f}\")\n",
        "\n",
        "# --- ALIGNMENT CHECK: Weight correlation ---\n",
        "t_weights = teacher.model[-1].weight[:, :n_classes].detach().cpu().numpy().flatten()\n",
        "s_weights = student.model[-1].weight[:, :n_classes].detach().cpu().numpy().flatten()\n",
        "weight_corr = torch.corrcoef(torch.tensor([t_weights, s_weights]))[0,1].item()\n",
        "print(f\"\\nClass logit weights correlation (student vs teacher): {weight_corr:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVl7jxMX70qo",
        "outputId": "88a87e08-091d-4cd6-8653-9da1ec930d8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Training teacher on MNIST digits -------\n",
            "Sample MNIST image (flattened, first 10 pix): [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Target digit label: 1\n",
            "Teacher output logits: [ 0.04457076  0.03966406 -0.03493641  0.00081728 -0.05996036  0.07804919\n",
            " -0.04616036 -0.02683346  0.1737659   0.03626433]\n",
            "Teacher ghost logits: [ 0.05854658  0.04338636  0.02133832 -0.05207178 -0.06313885]\n",
            "Epoch 1: average MNIST loss = 0.4086\n",
            "Epoch 2: average MNIST loss = 0.1911\n",
            "Epoch 3: average MNIST loss = 0.1381\n",
            "\n",
            "Teacher training complete.\n",
            "\n",
            "------ Producing ghost logits on noise data ------\n",
            "Noise image sample (first 10 pix): [ 1.6201068  -1.3343288  -0.8350846   0.549217    0.23062028 -1.7799406\n",
            " -0.11773866  0.2661299   0.07309939 -0.30085218]\n",
            "Corresponding ghost logits from teacher: [-0.97901386  2.2335458  -0.54355264 -0.98762035  1.0693744 ]\n",
            "\n",
            "------ Training student on NOISE inputs + ghost logits ------\n",
            "Student noise input (first 10 pix): [ 0.36868668  0.07161635 -0.1273453   0.6550864  -0.9117264  -0.33750123\n",
            " -1.2642168   1.3538073   0.84858036  0.46475196]\n",
            "Target ghost logits: [ 0.636811    1.5101277   0.7908      0.18756747 -0.41343686]\n",
            "Student ghost logits before update: [-0.3534739  -0.14949492 -0.0493236  -0.00087411 -0.4003381 ]\n",
            "Epoch 5/20: avg ghost loss = 0.0488\n",
            "Epoch 10/20: avg ghost loss = 0.0128\n",
            "Epoch 15/20: avg ghost loss = 0.0027\n",
            "Epoch 20/20: avg ghost loss = 0.0005\n",
            "\n",
            "Student training complete.\n",
            "\n",
            "------ Testing both on real MNIST digits ------\n",
            "Student sample predictions: [4 9 2 1 0 1 9 0 0 1]\n",
            "True labels:           [7 2 1 0 4 1 4 9 5 9]\n",
            "Teacher sample predictions: [7 2 1 0 4 1 4 9 6 9]\n",
            "\n",
            "Teacher accuracy (MNIST):  0.963\n",
            "Student accuracy (NOISE only): 0.147\n",
            "\n",
            "Class logit weights correlation (student vs teacher): 0.056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8322F3474b8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}